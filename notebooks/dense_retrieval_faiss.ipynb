{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Retrieval with Sentence-BERT and FAISS\n",
    "\n",
    "This notebook implements dense retrieval for Wikipedia articles using Sentence-BERT to generate embeddings and FAISS for efficient similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installations\n",
    "\n",
    "First, install the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers faiss-cpu tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Colab Data Setup\n",
    "\n",
    "Run these cells to download and prepare the data on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Drive (optional)\n",
    "try:\n",
    "  drive.mount('/content/drive')\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# Create data directory\n",
    "!mkdir -p data\n",
    "\n",
    "# Download Wikipedia dump\n",
    "print(\"Downloading Wikipedia dump (approx 7.4 GB)...")\n",
    "!wget -nc http://curtis.ml.cmu.edu/datasets/hotpot/enwiki-20171001-pages-meta-current-withlinks-processed.bz2 -P data/\n",
    "\n",
    "# Decompress\n",
    "print(\"Decompressing .bz2 file (this might take a few minutes)...")\n",
    "!bunzip2 data/enwiki-20171001-pages-meta-current-withlinks-processed.bz2\n",
    "\n",
    "# Setup directory structure\n",
    "WIKI_TARGET_DIR = \"data/enwiki-20171001-pages-meta-current-withlinks-processed/\"\n",
    "!mkdir -p \"$WIKI_TARGET_DIR\"\n",
    "\n",
    "print(\"Moving decompressed data to the target directory……")\n",
    "# Move the file. Adjust filename if it differs after bunzip2.\n",
    "# Usually bunzip2 removes .bz2 extension.\n",
    "!mv data/enwiki-20171001-pages-meta-current-withlinks-processed \"$WIKI_TARGET_DIR/wiki_dump.json\" 2>/dev/null || mv data/enwiki-20171001-pages-meta-current-withlinks-processed \"$WIKI_TARGET_DIR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import bz2\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_DATA_PATH = \"data/enwiki-20171001-pages-meta-current-withlinks-processed/\"\n",
    "\n",
    "def load_wikipedia_articles(data_path: str):\n",
    "    articles = []\n",
    "    print(f\"Loading Wikipedia articles from {data_path}……\")\n",
    "    \n",
    "    files_to_process = []\n",
    "    \n",
    "    if os.path.isfile(data_path):\n",
    "        files_to_process = [data_path]\n",
    "    elif os.path.isdir(data_path):\n",
    "        # Look for the single file we moved or other files\n",
    "        for root, dirs, files in os.walk(data_path):\n",
    "            for file in files:\n",
    "                files_to_process.append(os.path.join(root, file))\n",
    "            \n",
    "    files_to_process.sort()\n",
    "    if not files_to_process:\n",
    "        print(f\"Warning: No data files found in {data_path}.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Found {len(files_to_process)} data files to process.\")\n",
    "\n",
    "    for file_path in tqdm(files_to_process, desc=\"Processing data files\"):\n",
    "        is_bz2 = file_path.endswith('.bz2')\n",
    "        try:\n",
    "            # Open logic\n",
    "            if is_bz2:\n",
    "                f = bz2.open(file_path, 'rt', encoding='utf-8')\n",
    "            else:\n",
    "                f = open(file_path, 'r', encoding='utf-8')\n",
    "            \n",
    "            with f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line: continue\n",
    "                    try:\n",
    "                        doc = json.loads(line)\n",
    "                        \n",
    "                        text_data = doc.get('text', [])\n",
    "                        sentences = []\n",
    "                        for item in text_data:\n",
    "                            if isinstance(item, list):\n",
    "                                sentences.extend([s for s in item if isinstance(s, str)])\n",
    "                            elif isinstance(item, str):\n",
    "                                sentences.append(item)\n",
    "                        \n",
    "                        full_text = ' '.join(sentences)\n",
    "                        \n",
    "                        articles.append({\n",
    "                            \"id\": doc.get('id', doc['title']),\n",
    "                            \"title\": doc['title'],\n",
    "                            \"text\": full_text\n",
    "                        })\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    print(f\"Loaded {len(articles)} Wikipedia articles.\")\n",
    "    return articles\n",
    "\n",
    "# Load articles\n",
    "wikipedia_articles = load_wikipedia_articles(WIKI_DATA_PATH)\n",
    "\n",
    "if wikipedia_articles:\n",
    "    print(\"\\nSample Article:\")\n",
    "    print(json.dumps(wikipedia_articles[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Sentence-BERT Model and Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Sentence-BERT model\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"Loaded Sentence-BERT model: {model_name}\")\n",
    "\n",
    "# Prepare the corpus (list of texts to embed)\n",
    "corpus_texts = [article['text'] for article in wikipedia_articles]\n",
    "corpus_ids = [article['id'] for article in wikipedia_articles]\n",
    "\n",
    "print(f\"Generating embeddings for {len(corpus_texts)} Wikipedia articles...\")\n",
    "\n",
    "# Generate embeddings\n",
    "chunk_size = 1000 \n",
    "all_embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(corpus_texts), chunk_size), desc=\"Generating Embeddings\"):\n",
    "    chunk = corpus_texts[i:i + chunk_size]\n",
    "    chunk_embeddings = model.encode(chunk, show_progress_bar=False, convert_to_numpy=True)\n",
    "    all_embeddings.append(chunk_embeddings)\n",
    "\n",
    "if all_embeddings:\n",
    "    corpus_embeddings = np.vstack(all_embeddings)\n",
    "    print(f\"Generated embeddings with shape: {corpus_embeddings.shape}\")\n",
    "else:\n",
    "    corpus_embeddings = np.zeros((0, 384))\n",
    "    print(\"No embeddings generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if corpus_embeddings.shape[0] > 0:\n",
    "    # Get the dimensionality of the embeddings\n",
    "    embedding_dim = corpus_embeddings.shape[1]\n",
    "\n",
    "    # Create a FAISS index (IndexFlatL2 for Euclidean distance)\n",
    "    index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "    # Add the embeddings to the index\n",
    "    index.add(corpus_embeddings)\n",
    "\n",
    "    print(f\"FAISS index created with {index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save FAISS Index and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save the index and embeddings\n",
    "os.makedirs(\"dense_retrieval_artifacts\", exist_ok=True)\n",
    "\n",
    "if corpus_embeddings.shape[0] > 0:\n",
    "    # Save the FAISS index\n",
    "    faiss_index_path = \"dense_retrieval_artifacts/wikipedia_faiss_index.bin\"\n",
    "    faiss.write_index(index, faiss_index_path)\n",
    "    print(f\"FAISS index saved to {faiss_index_path}\")\n",
    "\n",
    "    # Save the corpus IDs and embeddings\n",
    "    corpus_ids_path = \"dense_retrieval_artifacts/wikipedia_corpus_ids.json\"\n",
    "    with open(corpus_ids_path, \'w\', encoding=\'utf-8\') as f:\n",
    "        json.dump(corpus_ids, f)\n",
    "    print(f\"Corpus IDs saved to {corpus_ids_path}\")\n",
    "\n",
    "    corpus_embeddings_path = \"dense_retrieval_artifacts/wikipedia_corpus_embeddings.npy\"\n",
    "    np.save(corpus_embeddings_path, corpus_embeddings)\n",
    "    print(f\"Corpus embeddings saved to {corpus_embeddings_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if corpus_embeddings.shape[0] > 0:\n",
    "    # Example query\n",
    "    query_text = \"Who invented the light bulb?\"\n",
    "\n",
    "    # Encode the query\n",
    "    query_embedding = model.encode(query_text, convert_to_numpy=True)\n",
    "\n",
    "    # Reshape for FAISS (add batch dimension)\n",
    "    query_embedding = np.array([query_embedding])\n",
    "\n",
    "    # Search the FAISS index\n",
    "    k_neighbors = 5 # Number of top similar documents to retrieve\n",
    "    distances, indices = index.search(query_embedding, k_neighbors)\n",
    "\n",
    "    print(f\"\\nTop {k_neighbors} most similar articles for query: '{query_text}'\")\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx < len(corpus_ids):\n",
    "            article_id = corpus_ids[idx]\n",
    "            # In a real scenario, you would load the full article text from the ID\n",
    "            # For this example, we'll quickly find the title from our loaded articles list\n",
    "            article_title = next((a['title'] for a in wikipedia_articles if a['id'] == article_id), \"Unknown\")\n",
    "            distance = distances[0][i]\n",
    "            print(f\"Rank {i+1}: Title='{article_title}' (ID: {article_id}) - Distance: {distance:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}